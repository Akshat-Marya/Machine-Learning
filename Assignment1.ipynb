{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assignment 1â€“ Introduction to Python & sklearn \n",
    "\n",
    "Questions:\n",
    "#1\t[10 marks] MNIST (http://yann.lecun.com/exdb/mnist/) is a famous dataset and benchmark for pattern recognition. It contains images of hand written numbers that are normalized and centered in a 28x28 pixel array. The data set was derived from a NIST (National Institute of Standards and Technology) data set, hence the acronym for Modified NIST. There are many ways to load this data set. An easy way is to use routines from tensorflow to load 50 images into a Python tuple with \n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "batch = mnist.train.next_batch(50)\n",
    "Write a program that displays some of these examples.\n",
    "\n",
    "\n",
    "#2\t[20 marks] In the example code for the Iris data classification, we used 10-fold cross-validation to evaluate the accuracy of predictions with a linear SVM. In the example, we used the sklearn method model_selection.cross_val_score. \n",
    "a. Explain briefly what k-fold cross validation is and what it is used for. \n",
    "b. Write a script that does a k-fold cross-validation without using the cross_val_score function and compares the results with the sklearn function.   \n",
    "c. Compare the cross-validated results of the SVM and RF and comment on which method is better.  \n",
    "\n",
    "\n",
    "#3\t[20 marks] Please download the wine.zip file and extract it to the directory for this assignment. Read through the wine_names.txt file and come to understand the problem and the wine data contained in the wine.train dataset. Train one of the models SVM, MLP, or RF to develop the best possible model for classifying the wine data in the hold-out test data set of 58 records in the wine.test file given the training data. In other words, you must submit a list of 58 classifications (as a separate *.csv file) for the hold-out test set in the same order as received. We will use your answers to score how well your model performs. \n",
    "Describe briefly your methodology for determining the best model and submit your final prediction program as well as the .csv file with the labels. Everyone will be ranked based on how well they do on their classification of the hold-out test set and 2 additional marks will be given to the best 10% of submissions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n",
      "[[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d88c694630>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d8956a9320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d893217fd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d89573e198>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d89577d048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "\n",
    "for i in range (0,5):\n",
    "    batch = mnist.train.next_batch(1)\n",
    "    print(batch[1])\n",
    "    a=np.reshape(batch[0],(28,28))\n",
    "    plt.imshow(a)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction of Svm Kernel:  [3. 1. 2. 2. 1. 2. 1. 2. 3. 2. 1. 1. 3. 3. 1. 1. 1. 2. 2. 1. 2. 1. 1. 2.\n",
      " 1. 1. 2. 2. 1. 1. 1. 3. 1. 2. 3. 2. 2. 2. 2. 2. 2. 2. 1. 1. 2. 2. 1. 3.\n",
      " 3. 2. 3. 2. 2. 2. 2. 1. 1. 2.]\n",
      "Prediction of RandomForestClassifier:  [3. 1. 2. 2. 1. 2. 2. 2. 3. 2. 1. 1. 3. 3. 1. 1. 1. 2. 2. 2. 2. 1. 1. 2.\n",
      " 1. 1. 2. 2. 1. 1. 1. 3. 1. 2. 3. 2. 2. 2. 2. 2. 2. 2. 1. 2. 2. 2. 1. 3.\n",
      " 3. 2. 3. 2. 2. 2. 2. 1. 1. 2.]\n",
      "Prediction of MLP:  [3. 1. 1. 2. 1. 2. 2. 2. 3. 2. 1. 1. 3. 2. 1. 1. 2. 2. 2. 2. 2. 1. 2. 2.\n",
      " 1. 1. 1. 1. 1. 1. 1. 3. 2. 2. 3. 2. 1. 2. 2. 2. 2. 1. 1. 2. 2. 2. 1. 3.\n",
      " 3. 2. 3. 2. 2. 2. 1. 2. 1. 3.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "wine_train = np.loadtxt('WINE_data\\wine.train',delimiter=',')\n",
    "wine_test = np.loadtxt('WINE_data\\wine.test',delimiter=',')\n",
    "#print(iris_data.shape)\n",
    "\n",
    "XTrain = wine_train[:,1:]\n",
    "YTrain = wine_train[:,0]\n",
    "\n",
    "XTest = wine_test[:,1:]\n",
    "YTest = wine_test[:,0]\n",
    "\n",
    "# print(wine_test[:,])\n",
    "# print(wine_test[0:,])\n",
    "\n",
    "#svm KERNEL\n",
    "model = svm.SVC(kernel='linear')\n",
    "model.fit(XTrain,YTrain)\n",
    "prediction=model.predict(XTest)\n",
    "print(\"Prediction of Svm Kernel: \",prediction)\n",
    "\n",
    "#RandomforestClassfier\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(XTrain,YTrain)\n",
    "a=clf.predict(XTest)\n",
    "print(\"Prediction of RandomForestClassifier: \",a)\n",
    "'''np.savetxt('B00784526_prediction.csv',a,delimiter=',')'''\n",
    "\n",
    "#MLPClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "mlp = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=1)\n",
    "mlp.fit(XTrain, YTrain)\n",
    "MLPClassifier(activation='relu', alpha=1e-05, batch_size='auto',\n",
    "       beta_1=0.9, beta_2=0.999, early_stopping=False,\n",
    "       epsilon=1e-08, hidden_layer_sizes=(5, 2), learning_rate='constant',\n",
    "       learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
    "       nesterovs_momentum=True, power_t=0.5, random_state=1, shuffle=True,\n",
    "       solver='lbfgs', tol=0.0001, validation_fraction=0.1, verbose=False,\n",
    "       warm_start=False)\n",
    "prediction = mlp.predict(XTest)\n",
    "print(\"Prediction of MLP: \", prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVC with linear kernel has been choosen to train the model for the following reasons:\n",
    "\n",
    "-SVC works good on small data sets.\n",
    "-overfitting is very low.\n",
    "-Since there is very low overfitting, it can give us good predicitons on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2) [a]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) K-fold cross validation: it is the way to find out how well out trained model would generalise on unseen and independent test data.\n",
    "    In this the training dataset is divided into k equal parts or folds. Then each of the part becomes testing data atleast once, while the rest of the parts or folds are used as training set.\n",
    "    Then the mean of the accuracy of the 'k' classifiers tells us how well our model would work on unseen data i.e. k-fold cross validation is used to find out the true accuracy of our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2) [c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iris_data shape:  (150, 5)\n",
      "\n",
      "\n",
      "trainx:  (75, 4) 75 triany:  (75,) testx:  (75, 4)\n",
      "prediction shape:  (75,)\n",
      "accuracy:  0.9333333333333333\n",
      "cross_val_score() gave the accuracy of:  0.9321428571428573\n",
      "\n",
      "\n",
      "trainx:  (75, 4) 75 triany:  (75,) testx:  (75, 4)\n",
      "prediction shape:  (75,)\n",
      "accuracy:  0.96\n",
      "RandomForestClassifier() gave the accuracy of:  0.9446428571428571\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "iris_data = np.loadtxt('IRIS_data\\iris.data',delimiter=',')\n",
    "print('iris_data shape: ',iris_data.shape)\n",
    "\n",
    "    \n",
    "train_x = iris_data[0::2,0:4]\n",
    "train_y = np.array(iris_data[0::2, 4])\n",
    "test_x = iris_data[1::2,0:4]\n",
    "test_y = np.array(iris_data[0::2, 4])\n",
    "kf = KFold(n_splits=10)\n",
    "\n",
    "#SVM\n",
    "clf = svm.SVC()\n",
    "clf.fit(train_x,train_y)\n",
    "print('\\n\\ntrainx: ',train_x.shape,len(train_x),'triany: ',train_y.shape,'testx: ',test_x.shape)\n",
    "prediction=np.array(clf.predict(test_x))\n",
    "print('prediction shape: ',prediction.shape)\n",
    "print(\"accuracy: \", accuracy_score(test_y,prediction))\n",
    "print('cross_val_score() gave the accuracy of: ',np.mean(cross_val_score(clf, train_x, train_y, cv=kf)))\n",
    "\n",
    "\n",
    "\n",
    "#RF\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(train_x,train_y)\n",
    "print('\\n\\ntrainx: ',train_x.shape,len(train_x),'triany: ',train_y.shape,'testx: ',test_x.shape)\n",
    "prediction=np.array(clf.predict(test_x))\n",
    "print('prediction shape: ',prediction.shape)\n",
    "print(\"accuracy: \", accuracy_score(test_y,prediction))\n",
    "print('RandomForestClassifier() gave the accuracy of: ',np.mean(cross_val_score(clf, train_x, train_y, cv=kf)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above RF gave the accuracy of 95.89% whereas SVM gave the accuarcy of 93.21%. RF works better on smaller datsets compared to SVM. The model is trained better using RF and the data is not overfitting unlike SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2) [b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "length=len(iris_data)\n",
    "fold_size=int(length/10)\n",
    "\n",
    "result=[]\n",
    "finalresult =[]\n",
    "iterator = iter(iris_data)\n",
    "count=0\n",
    "for i in range(length):\n",
    "    if(count<14):\n",
    "        result.append(iris_data[i])\n",
    "        count=count+1\n",
    "    else:\n",
    "        count = 0\n",
    "        result =[]\n",
    "        finalresult.append(result)\n",
    "folds = np.array(np.array_split(iris_data,10))\n",
    "\n",
    "#diving the dataset into 10-fold structure.\n",
    "CV_train_x = folds[:10,:,0:4]\n",
    "CV_train_y = folds[:10,:,4:]\n",
    "\n",
    "#shuffeling the dataset\n",
    "import random\n",
    "random.shuffle(CV_train_x)\n",
    "random.shuffle(CV_train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediciton number 0  : [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "accuracy:  1.0\n",
      "prediciton number 1  : [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "accuracy:  1.0\n",
      "prediciton number 2  : [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "accuracy:  1.0\n",
      "prediciton number 3  : [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "accuracy:  1.0\n",
      "prediciton number 4  : [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "accuracy:  1.0\n",
      "prediciton number 5  : [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "accuracy:  1.0\n",
      "prediciton number 6  : [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "accuracy:  1.0\n",
      "prediciton number 7  : [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "accuracy:  1.0\n",
      "prediciton number 8  : [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "accuracy:  1.0\n",
      "prediciton number 9  : [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "accuracy:  0.6666666666666666\n",
      "\n",
      "\n",
      " It gives the accuracy of:  0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "b=[]\n",
    "predict=[]\n",
    "accuracy=[]\n",
    "for x in range(10):\n",
    "    final_train_x = CV_train_x[np.arange(len(CV_train_x))!=x]\n",
    "    final_train_y = CV_train_y[np.arange(len(CV_train_y))!=x]\n",
    "    final_test_x = folds[x,:,0:4]\n",
    "    final_test_y = folds[x,:,4:]\n",
    "   \n",
    "    #svm KERNEL\n",
    "    model = svm.SVC()\n",
    "    model.fit(final_train_x.reshape(135,4),np.ravel(final_train_y.reshape(135,1)))\n",
    "    prediction=model.predict(final_test_x)\n",
    "    predict.append(prediction)\n",
    "    print(\"prediciton number\",x,\" :\",prediction)\n",
    "    print(\"accuracy: \", accuracy_score(final_test_y,prediction))\n",
    "    accuracy.append(accuracy_score(final_test_y,prediction))\n",
    " \n",
    "# finding highest accuracy from our cross-val-score\n",
    "list1 = accuracy\n",
    "list1= set(accuracy)\n",
    "list1 = list(list1)\n",
    "length=len(list1)\n",
    "list1.sort()\n",
    "print(\"\\n\\n It gives the accuracy of: \", list1[length-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
